{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes básicos\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from utils.funciones import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer dataset de entrenamiento\n",
    "\n",
    "# y_train es una lista con dos columnas, id y estado de la bomba\n",
    "y_train_filename=\"datos/y_train_data.csv\"\n",
    "df_ytrain=pd.read_csv(y_train_filename)\n",
    "\n",
    "\n",
    "# x_train es una lista con los datos de cada bomba, sin el estado.\n",
    "x_train_filename=\"datos/x_train_data.csv\"\n",
    "df_xtrain=pd.read_csv(x_train_filename)\n",
    "\n",
    "\n",
    "# segundo dataset que se usará en el proceso de machine learning\n",
    "x_test_filename=\"datos/x_test_data.csv\"\n",
    "df_xtest=pd.read_csv(x_test_filename)\n",
    "df_xtest.set_index('id')\n",
    "df_xtest.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# Fusionamos nuestras dos primeras bases de datos\n",
    "df_train=df_xtrain.merge(df_ytrain,how='inner',on='id',sort=True)\n",
    "df_train.set_index('id')\n",
    "df_train.sort_index(inplace=True)\n",
    "\n",
    "# Comprobamos valores duplicados y llenamos NAN\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_xtest.drop_duplicates(inplace=True)\n",
    "\n",
    "df_train=fill_object_nan(df_train)\n",
    "df_xtest=fill_object_nan(df_xtest)\n",
    "\n",
    "# Introducimos columnas de información para el date recorded.\n",
    "def add_date_columns(df):\n",
    "    df[\"date_recorded\"]=pd.to_datetime(df['date_recorded'],format='%Y-%m-%d',errors='coerce')\n",
    "    df[\"date_recorded_year\"]=df[\"date_recorded\"].dt.year\n",
    "    df[\"date_recorded_month\"]=df[\"date_recorded\"].dt.month\n",
    "\n",
    "    # Establecemos 1999 como año de cosntrucción de aquellos bombas que no se conocen.\n",
    "    df.construction_year=np.where(df.construction_year>0,df.construction_year,1999)\n",
    "    df.population=np.where(df.population>0,df.population,\\\n",
    "        round(np.median(df.population.loc[df.population>0].mean()),0))\n",
    "    return df\n",
    "\n",
    "df_train=add_date_columns(df_train)\n",
    "df_xtest=add_date_columns(df_xtest)\n",
    "\n",
    "for col in tipos_col(df_train)[\"obj\"]:\n",
    "    # simplificamos las variables categóricas, limitándolas a los diez valores por variable con presencia de > 2%.\n",
    "    df_train[col] = agrupa_categoricas(df_train,col,10,0)\n",
    "    \n",
    "    # conservamos mismos tipos escogidos en xtest\n",
    "    if col in df_xtest.columns:\n",
    "        valores_categoricas=df_train[col].unique()\n",
    "        df_xtest[col] = df_xtest[col].apply(lambda x: x if x in valores_categoricas else 'unknown')\n",
    "    \n",
    "# Simplificamos el target para obtener 2 en primicia\n",
    "df_train['status_group_simpl']=df_train['status_group']\n",
    "df_train['status_group_simpl']=np.where(df_train['status_group_simpl']=='functional','functional','non functional')\n",
    "\n",
    "#Substutimos valores extra por \"desconocidos\"\n",
    "df_train.replace(\"other\",\"unknown\",inplace=True)\n",
    "df_xtest.replace(\"other\",\"unknown\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rucufuto\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\Rucufuto\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexing.py:1965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj._check_is_chained_assignment_possible()\n"
     ]
    }
   ],
   "source": [
    "\" Existen muchos valores con longitud y latitud cero. sin embargo, tienen una región asignada. Por ello, vamos a asignarles valores al azar entre los puntos de cada región\"\n",
    "\n",
    "df_train_local=df_train.loc[df_train[\"longitude\"]!=0]\n",
    "df_xtest_local=df_xtest.loc[df_xtest[\"longitude\"]!=0]\n",
    "\n",
    "regiones_maximas=df_train_local[[\"gps_height\",\"longitude\",\"latitude\",\"region_code\"]].groupby(by=[\"region_code\"]).max().add_suffix(\"_max\")\n",
    "regiones_minimas=df_train_local[[\"gps_height\",\"longitude\",\"latitude\",\"region_code\"]].groupby(by=[\"region_code\"]).min().add_suffix(\"_min\")\n",
    "\n",
    "regiones=regiones_maximas.join(regiones_minimas)\n",
    "\n",
    "#inputamos valores de colocación basados en región en una nueva variable (df_train_one)\n",
    "df_train_zero=df_train.loc[df_train[\"longitude\"]==0]\n",
    "df_xtest_zero=df_xtest.loc[df_xtest[\"longitude\"]==0]\n",
    "\n",
    "var_imputables=[\"gps_height\",\"longitude\",\"latitude\"]\n",
    "\n",
    "for region in regiones.index:\n",
    "    for variable in var_imputables:\n",
    "\n",
    "        min=regiones.loc[[region]][variable+\"_min\"].values[0]\n",
    "        max=regiones.loc[[region]][variable+\"_max\"].values[0]\n",
    "\n",
    "        tam_train=df_train_zero[variable].loc[df_train_zero[\"region_code\"]==region].shape\n",
    "        tam_xtest=df_xtest_zero[variable].loc[df_xtest_zero[\"region_code\"]==region].shape\n",
    "\n",
    "        valores_train=np.random.random_sample(size=tam_train)*(max-min)+np.ones(tam_train)*min\n",
    "        valores_xtest=np.random.random_sample(size=tam_xtest)*(max-min)+np.ones(tam_xtest)*min\n",
    "\n",
    "        df_train_zero[variable].loc[df_train_zero[\"region_code\"]==region]=valores_train\n",
    "        df_xtest_zero[variable].loc[df_xtest_zero[\"region_code\"]==region]=valores_xtest\n",
    "\n",
    "df_train_loc_imput=df_train_zero[[\"region_code\"]+var_imputables]\n",
    "df_xtest_loc_imput=df_xtest_zero[[\"region_code\"]+var_imputables]\n",
    "\n",
    "df_train_one=df_train.copy()\n",
    "df_xtest_one=df_xtest.copy()\n",
    "\n",
    "df_train_one.loc[df_train_zero.index, var_imputables]=df_train_zero[var_imputables]\n",
    "df_xtest_one.loc[df_xtest_zero.index, var_imputables]=df_xtest_zero[var_imputables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archivo de datos train gps encontrado\n",
      "leyendo de datos/df_train_one_gps.pkl\n",
      "archivo de datos xtest gps encontrado\n",
      "leyendo de datos/df_xtest_one_gps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Añadimos quién es el pozo más cercano. \n",
    "# Para esto nos basaremos UNICAMENTE en los pozos de la misma región del set de entrenamiento. \n",
    "# Esto se dará incluso si estamos en el set de test.\n",
    "# Es un proceso largo, puede durar más de una hora. Por eso, miraré si he calculado ya el punto.\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "ruta='datos/'\n",
    "\n",
    "# Miramos si ya he trabajado estos datos antes. Si no, los creamos. Esta operación puede llevar hasta una hora.\n",
    "archivo_gps_train='df_train_one_gps.pkl'\n",
    "path= Path(ruta+archivo_gps_train)\n",
    "if path.is_file():\n",
    "    print(\"archivo de datos train gps encontrado\")\n",
    "    print(f\"leyendo de {ruta+archivo_gps_train}\")\n",
    "    df_train_one_gps=joblib.load(ruta+archivo_gps_train)\n",
    "    df_train_one_gps=df_train_one_gps.set_index('id')\n",
    "    df_train_one=df_train_one.join(df_train_one_gps['dist_mas_cercano'],how='inner')\n",
    "else:\n",
    "    df_train_one[\"dist_mas_cercano\"]=lugar_mas_cercano(df_train_one[[\"longitude\",\"latitude\",\"region_code\"]],\"gps\",True)\n",
    "    joblib.dump(df_train_one, ruta+archivo_gps_train)\n",
    "\n",
    "# Repetimos el paso con los datos del dataset objetivo.\n",
    "archivo_gps_xtest='df_xtest_one_gps.pkl'\n",
    "path= Path(ruta+archivo_gps_xtest)\n",
    "if path.is_file():\n",
    "    print(\"archivo de datos xtest gps encontrado\")\n",
    "    print(f\"leyendo de {ruta+archivo_gps_xtest}\")\n",
    "    df_xtest_one_gps=joblib.load(ruta+archivo_gps_xtest)\n",
    "    df_xtest_one=df_xtest_one.join(df_xtest_one_gps['dist_mas_cercano'],how='inner')\n",
    "else:\n",
    "    df_xtest_one[\"dist_mas_cercano\"]=lugar_mas_cercano_xtest(df_train_one[[\"longitude\",\"latitude\",\"region_code\"]],\\\n",
    "        df_xtest_one[[\"longitude\",\"latitude\",\"region_code\"]],\"gps\",True)\n",
    "    joblib.dump(df_xtest_one, ruta+archivo_gps_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos variables, guardamos indices como indice.\n",
    "\n",
    "df_train_one.set_index('id',inplace=True)\n",
    "df_xtest_one.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Pickle labelEncoder dump executed ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convertimos o serializamos las clases en formato pickle pkl\n",
    "joblib.dump(df_train_one, \"datos/df_train_datos.pkl\")\n",
    "joblib.dump(df_xtest_one, \"datos/df_xtest_datos.pkl\")\n",
    "joblib.dump(regiones,'datos/regiones.pkl')\n",
    "print(\" --- Pickle labelEncoder dump executed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta='datos/'\n",
    "archivo_xtrain='df_train_datos.pkl'\n",
    "archivo_xtest='df_xtest_datos.pkl'\n",
    "df_train_one=joblib.load(ruta+archivo_xtrain)\n",
    "df_xtest_one=joblib.load(ruta+archivo_xtest)\n",
    "\n",
    "# Categorías a trabajar\n",
    "lista_numericas=['amount_tsh', 'construction_year', 'longitude', 'latitude', 'gps_height','population','dist_mas_cercano']\n",
    "lista_categoricas=[\"waterpoint_type\",\"source_type\",\"quality_group\",\"quantity_group\",\"payment_type\",\"extraction_type_class\",\"permit\",\"scheme_name\",\"funder\",\"installer\"]\n",
    "listas_targets=['status_group','status_group_simpl']\n",
    "\n",
    "lista_categoricas=[\"quantity_group\",\"extraction_type_class\",\"waterpoint_type\"]\n",
    "listas=[lista_numericas,lista_categoricas,listas_targets]\n",
    "listas=sum(listas,[])\n",
    "listas_test=sum([lista_numericas,lista_categoricas],[])\n",
    "\n",
    "df_train_two=df_train_one[listas]\n",
    "df_xtest_two=df_xtest_one[listas_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aplicamos dummies a df_train\n",
      "aplicamos dummies\n",
      "columna: quantity_group\n",
      "columna: extraction_type_class\n",
      "columna: waterpoint_type\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos dummies\n",
    "\n",
    "print(\"aplicamos dummies a df_train\")\n",
    "df_train_two_dm=aplica_dummies(df_train_two,lista_categoricas)\n",
    "# primero conseguido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aplicamos dummies a df_xtest\n",
      "aplicamos dummies\n",
      "columna: quantity_group\n",
      "columna: extraction_type_class\n",
      "columna: waterpoint_type\n",
      "ambos conseguidos\n"
     ]
    }
   ],
   "source": [
    "print(\"aplicamos dummies a df_xtest\")\n",
    "df_xtest_three=aplica_dummies(df_xtest_two,lista_categoricas)\n",
    "print(\"ambos conseguidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos en nueva df\n",
    "\n",
    "df_train_two_dm['target_trio']=df_train_two_dm['status_group'].replace(['functional', 'functional needs repair','non functional'],[0, 1,2])\n",
    "df_train_two_dm['target_duo']=df_train_two_dm['status_group_simpl'].replace(['functional', 'functional needs repair','non functional'],[0, 1,1])\n",
    "df_train_three=df_train_two_dm.drop(columns=['status_group','status_group_simpl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si hay columnas extra de dummmies en df_train_two que no hay en df_xtest, las añadimos ahora, con valor cero en todas ellas.\n",
    "\n",
    "col_comunes=set(df_train_three.columns).symmetric_difference(set(df_xtest_three.columns)).symmetric_difference(set(['target_duo','target_trio']))\n",
    "for col in col_comunes:\n",
    "    df_xtest_three[col] = 0\n",
    "\n",
    "col_sin_target=list(set(df_train_three.columns).intersection(set(df_xtest_three.columns)))\n",
    "col_sin_target.sort()\n",
    "col_con_target=col_sin_target[:]\n",
    "col_con_target.extend(['target_duo','target_trio'])\n",
    "\n",
    "df_train_three=df_train_three[col_con_target]\n",
    "df_xtest_three=df_xtest_three[col_sin_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Pickle labelEncoder dump executed ---\n"
     ]
    }
   ],
   "source": [
    "# Convertimos o serializamos las clases en formato pickle pkl\n",
    "\n",
    "joblib.dump(df_train_three, \"datos/df_train_datos2.pkl\")\n",
    "joblib.dump(df_xtest_three, \"datos/df_xtest_datos2.pkl\")\n",
    "print(\" --- Pickle labelEncoder dump executed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df2f563f9405832242b2cc9971b3350a36eba2cefa78a66d80eae6dbc2791b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
